%% hauptteil.tex


\chapter{\citetitle{li2019unified}}
\label{ch:MRC}
%% ==============================


%% ==============================
\section{Überblick}
%% ==============================
\label{ch:MRC:sec:Überblick}

\cite{li2019unified} setzen ihren einheitlichen Ansatz um, indem sie die Aufgaben NER und NNER als Ausprägungen der Machine Reading Comprehension (MRC) interpretieren. Die Problemstellung der Extraktion von Personennamen wird also als eine Frage-Antwort-Aufgabe formuliert, bei der als Antwort auf die Frage \emph{"Welche Person wird im Text erwähnt?"} eine oder mehrere Strecken des Eingabetextes mittels Start- und Endeankern markiert werden, die die Antworten enthalten. Analog kann für beliebige weitere Klassen verfahren werden. Als Basis für die Implementierung wird das BERT-Modell \parencite{devlin2019bert} verwendet. Da BERT zum Training zwei durch den speziellen [SEP]-Token getrennte Textsequenzen als Eingabe erwartet, können zum Zwecke des (N)NER-Trainings Paare aus \textsc{(Frage, Suchtext)} verwendet werden.

Um die gleichzeitige Anwendung für NER und NNER zu realisieren, werden zwei binäre Klassifikatoren auf den BERT Embeddings trainiert. Diese entscheiden für jedes Token des Suchtextes, ob es der Start bzw. das Ende einer Antwort im Sinne der formulierten Frage ist. Auf diese Weise lässt sich nicht nur einfache NER, sondern auch NNER, sowie die Erkennung möglicherweise überlappender Entitäten umsetzen. Um zuzuordnen, welche Paare aus Anfangs- und Endeanker zusammengehören, wird ein weiterer Klassifikator trainiert, der die dazugehörigen Wahrscheinlichkeiten vorhersagt.

%% ==============================
\section{Datenaufbereitung}
%% ==============================
\label{ch:MRC:sec:Datenaufbereitung}

Oft liegen Trainingsdaten für NER-Anwendungen im BIO-Format oder im CoNLL-Format vor. Für die Zwecke des MRC Frameworks entschieden sich \Citeauthor{li2019unified} für ein Zwischenformat, das ein Tupel aus \textsc{(Frage, Antwort, Kontext)} abbildet. Aus diesem Zwischenformat haben die Autoren mittels eines Konversionsscripts die Eingabedaten für ihre angepassten BERT-Datasets erstellt.

Bedauerlicherweise wird das besagte Zwischenformat zwar abstrakt beschrieben, die tatsächliche Implementierung jedoch nicht erklärt. So war es erforderlich, aus dem vorliegenden Quellcode\footnote{\url{https://github.com/ShannonAI/mrc-for-flat-nested-ner/}} die Funktionsweise zu rekonstruieren und ein zusätzliches Konversionsscript zu programmieren, das aus den deutschen Korpora das benötigte Format herstellt (\hyperref[app:conll2json]{Anhang \texttt{scripts/conll2json.py}}). Das so erzeugte Datenformat ist dargestellt in \autoref{lst:jsonexample}. Jeder Satz wird mittels der Schlüssel \verb|"context"| und \verb|"label"| codiert. Dabei enthält \verb|"context"| den Inhalt des vollständigen Satzes als Whitespace-Verkettung aller Tokens der Eingabedatei. \verb|"label"| enthält als Objekt alle vorhandenen Klassen der Named Entities als Schlüssel. Die Werte dieser Schlüssel sind Listen aus \verb|"<start>;<end>"| Strings, die den Indizes der entsprechenden Vorkommnisse innerhalb des \verb|"context"| entsprechen.

\begin{lstlisting}[caption={Beispiel aus GermEval2014 für das erforderliche JSON-Format}, language={Python}, label={lst:jsonexample}]
	{
		"context": "Aber Borussia Dortmund darf niemals einem allein gehören .",
		"label": {
			"ORG": [
				"1;2"
			],
			"LOC": [
				"2;2"
			]
		}
	}
\end{lstlisting}

Aus den Quelldateien der Autoren war nicht ersichtlich, ob und wie dieses Format unter Berücksichtigung von NNER angepasst werden musste. Aus diesem Grund wurde die Annahme getroffen, dass eine Verschachtelung wie in \autoref{lst:jsonexample} über die naive Angabe der Indizes bereits ausreichend ist.

%% ==============================
\chapter{Training für deutsche Korpora}
\label{ch:Training}
%% ==============================

%% ==============================
\section{Deutsche Korpora}
%% ==============================
\label{ch:Training:sec:Deutsche_Korpora}

Die Wahl der Korpora wurde vorrangig durch Verfügbarkeit und Kompatibilität mit dem gegebenen Framework geleitet. Mindestens ein Korpus sollte NNER abbilden können, weshalb die Entscheidung für GermEval2014 \parencite{germeval2014} getroffen wurde. Mit MultiNERD \parencite{multinerd} wurde ein Korpus betrachtet, das besonders durch die Vielzahl an ausgezeichneten Entityklassen interessant ist. Zusätzlich wurde WikiANN \parencite{wikiann} aufgenommen, allerdings hauptsächlich aus opportunistischen Gründen bezüglich Verfügbarkeit und Aufwand. Eine Übersicht der verwendeten Korpora ist in \autoref{tab:korpora} dargestellt.

Mit Europeana Newspapers \parencite{europeana} wurde ein weiteres Korpus in Betracht gezogen, jedoch konnte dieses nicht verwendet werden, da hier keine Trennung der Sätze stattfindet. Der Versuch, mittels Anwendung eines Satztokenizers die Trennung automatisiert vorzunehmen, lieferte keine brauchbaren Ergebnisse. Der Hauptgrund dafür ist vermutlich, dass die Datenquelle dieses Korpus eine OCR-basierte Digitalisierung von Zeitungsartikeln ist, die keine ausreichende Qualität bietet. Die Problematik der Satztzrennung wird von Neudecker ebenfalls beklagt.

\begin{table}[!htbp]
	\centering
	\caption{Verwendete deutsche Korpora}
	\label{tab:korpora}
	\begin{tabular}{@{}lllll@{}}
		\toprule
		\textbf{Korpus}   & \textbf{Sätze} & \textbf{Tokens}       & \textbf{Nested} & \textbf{Entityklassen} \\ \midrule
		GermEval2014    & 31,000         & \textgreater{}590,000 & Ja              & 4                       \\
		WikiANN         & 40,000         & \textgreater{}390,000 & Nein            & 3                       \\
		MultiNERD\footnotemark       & 156,800        & 2,700,000             & Nein            & 15                      \\ \bottomrule
		\end{tabular}
\end{table}
\footnotetext{Die hier gelistete Anzahl der Sätze und Tokens entspricht der Gesamtgröße des Korpus. Zur Verwendung in dieser Arbeit siehe \autoref{ch:Training}.}

%% ==============================
\section{Deutsches BERT-Modell}
%% ==============================
\label{ch:Training:sec:DeutschBERT}

Während meine früheren Arbeiten auf \verb|bert-base-german-cased| \parencite{bert-base-german-cased} basierten, fiel in dieser Arbeit die Entscheidung für das modernere \verb|gbert-base| \parencite{gbert}, das mit einer deutlich besseren Performance beworben wird. Letztenendes wurde diese Entscheidung aus rein pragmatischen Gründen getroffen, da bereits zu Beginn absehbar war, dass in dieser Arbeit lediglich ein Modell getestet werden kann. Gleichzeitig ist die Wahl des Basismodells nur eine der vielen Stellschrauben, die Einfluss auf die Performance des Frameworks haben. Einige dieser Stellschrauben werden im Folgenden an gegebener Stelle aufgezeigt, sowie in \autoref{ch:Ausblick} überblicksartig zusammengefasst.


%% ==============================
\section{Debugging}
%% ==============================
\label{ch:Training:sec:Debugging}

Im Repository der Autoren befinden sich Scripts, die eine Reproduktion der Ergebnisse aus \cite{li2019unified} ermöglichen sollen. Als Basis für die Anpassung an die deutschen Korpora wurde \href{https://github.com/ShannonAI/mrc-for-flat-nested-ner/blob/457b0759f7fd462d0abd0a23441726352716fff9/scripts/mrc_ner/reproduce/genia.sh}{\texttt{scripts/mrc\_ner/reproduce/genia.sh}} verwendet. Bevor eine angepasste Version dieses Scripts verwendet werden konnte, mussten jedoch zunächst diverse Versionskonflikte der installierten Python-Pakete behoben werden. Dieses Debugging war wie üblich sehr zeitaufwändig, da die produzierten Fehlermeldungen nicht immer einen offensichtlichen Hinweis auf das zugrundeliegende Problem lieferten. Die relevanten finalen Versionen der verwendeten Pakete sind im \hyperref[app:versions]{Anhang \texttt{versions.txt}} aufgeführt. Überraschenderweise zeigten sich große Diskrepanzen zwischen den auf diese Weise ermittelten Versionen und denen, die in der mitgelieferten \href{https://github.com/ShannonAI/mrc-for-flat-nested-ner/blob/457b0759f7fd462d0abd0a23441726352716fff9/requirements.txt}{\texttt{requirements.txt}} gelistet sind.

Nachdem Konflikte in der Python-Installation bereinigt waren, wurden die in \autoref{ch:MRC:sec:Datenaufbereitung} erläuterten Probleme der Datenformate offenbart. Im Zuge dessen musste ein Konversionsscript entwickelt werden (\hyperref[app:conll2json]{Anhang \texttt{scripts/conll2json.py}}). Aus den so erhaltenen JSON-Daten wurde mit \href{https://github.com/ShannonAI/mrc-for-flat-nested-ner/blob/457b0759f7fd462d0abd0a23441726352716fff9/ner2mrc/genia2mrc.py}{\texttt{ner2mrc/genia2mrc.py}} anschließend das benötigte MRC-Format generiert, das letztendlich die Eingabe für das Training darstellt.

Ein weiterer Faktor, der in \autoref{ch:MRC} bereits angedeutet wurde, sind die erforderlichen Formulierungen der Fragen bzw. Queries, anhand derer das Modell Antworten finden soll. Während die einfachste Methode darin besteht, ein einzelnes Wort als Query zu verwenden, kann ebenfalls kurze Umschreibung oder gar eine voll ausformulierte Frage zur Entityklasse genutzt werden. Weitere Untersuchungen zu diesem Aspekt finden sich in \cite{li2019unified}.

Ein erster Versuch, eine mitgelieferte Query-Definition der Autoren unverändert zu übernehmen, ließ das Training zwar erfolgreich abschließen. In der Evaluation wurde jedoch ein enttäuschender F1-Score von ca. 0.51 erreicht. Dies ist auf mangelndes Verständnis des Frameworks zurückzuführen, da beim zweiten Blick klar wurde, dass die unreflektiert übernommenen Queries auf Chinesisch formuliert waren und somit für ein deutsches Modell natürlich nicht nutzbar.

Ein weiteres Problem zeigte sich beim Versuch, das Modell für MultiNERD zu trainieren. Aufgrund der schieren Größe der Datenmenge (s. \autoref{tab:korpora}) war es nicht machbar, eine Konfiguration der Hyperparameter zu finden, die ein Training auf einer GPU in vertretbarer Zeit ermöglichte. Da die mitgelieferten Reproduktionsscripts ebenfalls Konfigurationen für ein Multi-GPU Setup enthielten, wurde zunächst versucht, diese entsprechend zu übernehmen. Es zeigten sich aber zu große Unterschiede zwischen den Serverarchitekturen der Autoren und dem mir zur Verfügung stehenden Server. Daher wurde der Versuch aufgegeben, das gesamte MultiNERD Korpus zu verwenden. Stattdessen wurde die Größe des Korpus auf \(\frac{1}{6}\) (ca. 26133 Sätze) reduziert, was ein Training auf einer GPU ermöglichte.

Alle finalen Trainings- und Evaluationsscripts können im Anhang eingesehen werden (\hyperref[app:train]{Anhang \texttt{train/}} und \hyperref[app:evaluate]{Anhang \texttt{evaluate/}}).


%% ==============================
\section{Anpassung der Queries}
%% ==============================
\label{ch:Training:sec:Anpassung_Queries}


Da GermEval2014 der erste für dieses Experiment verwendete Datensatz war, wurden zunächst Ein-Wort-Queries als Platzhalter eingefügt, um erste plausible Ergebnisse zu erhalten. Da es für den Fortschritt des Experimentes wichtiger erschien, andere Korpora zu untersuchen, wurde an dieser Stelle auf ein anschließendes Training mit angepassten Queries für GermEval2014 verzichtet. Da MultiNERD eine große Zahl an Entityklassen enthält und in \cite{multinerd} Definitionen bereitgestellt werden, wurden diese ins Deutsche übersetzt und als Queries verwendet. Für das anschließende Training mit WikiANN wurden ebenfalls die übersetzten Definitionen verwendet.

Nach dem erfolgten Training für alle drei Korpora stellte sich die Frage, ob ein erneuter Trainingsdurchlauf mit angepassten Queries für GermEval2014 in Betracht gezogen werden sollte. Die Erfahrung zeigte nun, dass die längeren Queries erheblichen Einfluss auf den Ressourcenbedarf des Trainings haben: Das Training mit vollständigen Definitionen erfordert ca. 50\% mehr Zeit und VRAM als das Training mit Ein-Wort-Queries. In Anbetracht der ohnehin erheblichen Trainingsdauer (vgl. \autoref{tab:evaluation}) wurde daher im Rahmen dieser Arbeit kein weiteres Training mit angepassten Queries durchgeführt.

Die finalen Queries können im Anhang eingesehen werden (\hyperref[app:queries]{Anhang \texttt{queries/}}).


%% ==============================
\chapter{Evaluation}
\label{ch:Evaluation}
%% ==============================

%% ==============================
\section{Übersicht der Trainingsergebnisse}
%% ==============================
\label{ch:Evaluation:sec:Übersicht_der_Trainingsergebnisse}

Es folgen in \autoref{tab:evaluation} die Ergebnisse der Evaluation des MRC Frameworks. Insgesamt sind die Ergebnisse vergleichbar mit denen englischsprachiger Korpora von \Citeauthor{li2019unified}. Speziell die mit MultiNERD erreichten Scores übertreffen diese teilweise sogar deutlich. Auch wenn die Performance auf GermEval2014 ungefähr 6\%\textendash8\% schlechter abschneidet, sind die Ergebnisse wesentlich besser als die ursprünglichen Systeme der GermEval Konferenz \parencite{germeval2014}. Außerdem sind sie vergleichbar mit moderneren Modellen wie \cite{riedl-pado-shootout}, die auf diesem Korpus trainiert wurden. Für das deutsche WikiANN gibt es kaum Vergleichswerte, jedoch wird in \cite{Schiesser_2023} ein etwas besserer F1-Score von 0.8892 berichtet.

Die Evaluation fand mit angepassten Skripten der Autoren statt (\hyperref[app:evaluate]{Anhang \texttt{evaluate/}}). Metriken werden gemäß \cite{li2019unified} als span-level micro-average berichtet.

\begin{table}[!htbp]
	\centering
	\caption{Evaluation des MRC Frameworks}
	\label{tab:evaluation}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{@{}l|ccc|ccc|c@{}}
		\specialrule{.1em}{0em}{0em}
		\textbf{Korpus}       & \multicolumn{3}{c|}{\textbf{dev}}                       & \multicolumn{3}{c|}{\textbf{test}}                  & \textbf{Trainingsdauer}                         \\
		\textbf{} & \textbf{f1}     & \textbf{precision} & \textbf{recall} & \textbf{f1} & \textbf{precision} & \textbf{recall} & \multicolumn{1}{c}{\textbf{(in Min.)}} \\
		\specialrule{.05em}{0em}{0em}
		GermEval2014    & 0.8255          & 0.8379             & 0.8136          & 0.8111          & 0.8369             & 0.7867          & 200                               \\
		WikiANN         & 0.8578          & 0.8625             & 0.8532          & 0.8604          & 0.8545             & 0.8665          & 223                               \\
		MultiNERD       & \textbf{0.8823} & \textbf{0.8986}    & \textbf{0.8665} & \textbf{0.8841} & \textbf{0.8994}    & \textbf{0.8692} & 644                               \\ \specialrule{.1em}{0em}{0em}
		\end{tabular}
	}
\end{table}

%% ==============================
\section{Interpretation der Trainingsergebnisse}
%% ==============================
\label{ch:Evaluation:sec:Interpretation_der_Trainingsergebnisse}

Es gibt verschiedene Erklärungsansätze für die Beobachtung, dass GermEval2014 deutlich schlechter abschneidet als die anderen Korpora. Einerseits ist es der einzige Datensatz des NNER-Tasks. Wegen der höheren Komplexität dieser Aufgabe sind die Ergebnisse grundsätzlich als Sonderfall zu betrachten. Zudem ist GermEval2014 auch das kleinste Korpus (vgl. \autoref{tab:korpora}), wodurch möglicherweise kein ausreichendes Training möglich war. Darüber hinaus war es, wie in \autoref{ch:Training:sec:Anpassung_Queries} beschrieben, das erste Korpus, auf dem trainiert wurde, und hatte lediglich Ein-Wort-Queries als Basis. Es ist mit Sicherheit davon auszugehen, dass dieser Faktor die Performance entscheidend reduziert hat.

WikiANN und MultiNERD sind maschinell generierte Silver-Standard-Korpora. Das führt zu einer großen Menge an Trainingsdaten, die jedoch nicht immer von hoher Qualität sind. Demzufolge ist die Qualität eines trainierten Modells nicht notwendigerweise besonders gut, obwohl die Evaluation dieses Modells positiv ausfällt.

Infolgedessen sind die Ergebnisse in dieser Form nur eingeschränkt vergleichbar. Um ein tatsächlich faires Bild der Performance des MRC Frameworks zu erhalten, sollten weitere Experimente durchgeführt werden. Mögliche Faktoren, die dabei berücksichtigt werden könnten, werden in \autoref{ch:Ausblick} aufgeführt.

%% ==============================
\chapter{Ausblick}
%% ==============================
\label{ch:Ausblick}

Trotz der aufschlussreichen Ergebnisse bezüglich der Performance des MRC Frameworks auf deutschen Korpora gibt es an vielen Stellen Bedarf für weitere Untersuchungen. Im Rahmen dieser Arbeit war vor allem angesichts der hohen Trainingsdauer nicht möglich, auf alle denkbaren Konfigurationen einzugehen. Im Folgenden werden einige Aspekte aufgeführt werden, von denen zu erwarten ist, dass sie großen Einfluss auf die Performance des Frameworks haben.

\begin{itemize}
	\item \textbf{Hyperparameter:} Die Anzahl der Epochen, die Batch-Size und die Lernrate sind offensichtliche Hyperparameter, mit denen experimentiert werden könnte. Mögliche Versuche sind allerdings direkt gebunden an die verfügbaren Ressourcen.
	\item \textbf{Queries:} Wie in \autoref{ch:Training:sec:Anpassung_Queries} beschrieben, ist die Formulierung der Queries ein entscheidender Faktor, der von \Citeauthor{li2019unified} genauer beleuchtet wurde. Allerdings wurde nicht im Speziellen die deutsche Sprache diskutiert. Obwohl zu vermuten ist, dass sich die Beobachtungen der Autoren auch auf das Deutsche übertragen lassen, bleibt dies zu überprüfen.
	\item \textbf{Korpora:} Da die in dieser Arbeit gewählten Korpora lediglich eine kleine Auswahl der verfügbaren Daten darstellen, ist es naheliegend, die Untersuchungen auf weitere Korpora auszubreiten, sowie eine technische Konfiguration zu erreichen, in der ein Training mit dem vollständigen MultiNERD Korpus möglich ist.
	\item \textbf{BERT-Modell:} Das verwendete Modell \verb|gbert-base| wirkt zwar vielversprechend, trotzdem ist es denkbar, dass andere Modelle, die bspw. auf BERT\textsubscript{LARGE} oder RoBERTa basieren, bessere Ergebnisse liefern. Eine andere Modellarchitektur, die überhaupt nicht auf BERT basiert, ist zwar theoretisch denkbar. Da jedoch das gesamte MRC Framework auf die BERT-Architektur ausgelegt ist, wäre dies nicht ohne erheblichen Aufwand umsetzbar.
\end{itemize}

